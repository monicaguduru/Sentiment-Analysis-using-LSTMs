# -*- coding: utf-8 -*-
"""third_question.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17xJquyR5g9Qrk-baOwcVjjXgQ4DAkIts
"""

from google.colab import drive
drive.mount('/content/drive')

from sklearn.utils import shuffle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from string import punctuation
from collections import Counter
import tensorflow as tf
from tensorflow.contrib import rnn

dataFrame = pd.read_csv('drive/My Drive/A1-Q3_Dataset/mrdata.tsv',delimiter='\t',encoding='utf-8')

dataFrame.head()

print(dataFrame.shape)

for i in dataFrame:
    print(i)

new_dataFrame = dataFrame
new_dataFrame = new_dataFrame.drop(columns="PhraseId")
new_dataFrame = new_dataFrame.drop(columns="SentenceId")

new_dataFrame.head()

new_dataFrame = shuffle(new_dataFrame)

sentiments = new_dataFrame.iloc[:, 1].values
phrases = new_dataFrame.iloc[:, 0].values

print(sentiments[1], phrases[1])

modified_phrases = []

for i in modified_phrases:
    x = ''.join([c for c in i if c not in punctuation])
    modified_phrases.append(x)

new_phrases = []
all_phrases = []
for x in modified_phrases:
    new_phrases.append(x.lower().split())
    for i in x.split():
        all_phrases.append(i.lower())

count_iterator = Counter(all_phrases)

sorted_dictionary = sorted(count_iterator, key=count_iterator.get, reverse=True)

dictionary_to_int = {x: i for i, x in enumerate(sorted_dictionary, 1)}

int_phrases = []
for x in new_phrases:
    int_phrases.append([dictionary_to_int[i] for i in x])

lens_phrases = Counter([len(x) for x in int_phrases])

print('Phrases of length zero is {}'.format(lens_phrases[0]))

l = 15

features = np.zeros((len(int_phrases), l), dtype=int)
for i, x in enumerate(int_phrases):
    if(len(x)!=0):
        features[i, -len(x):] = np.array(x)[:l]

train_len = int(features.shape[0] * 0.75)
train_features = features[:train_len]
train_labels = sentiments[:train_len]

test_features = features[train_len:]
test_labels = sentiments[train_len:]

print('Shape of training features {}'.format(train_features.shape))

batch_size = 1000
learning_rate = 0.001 
hidden_layer_size = 512 
number_of_layers = 1 
number_of_words = len(dictionary_to_int) + 1 
dropout_rate = 0.8 
embedding_size = 300 
epochs = 2

tf.reset_default_graph()

input_values = tf.placeholder(tf.int32, [None, None], name='input_values')
target_values = tf.placeholder(tf.int32, [None, None], name='target_values')

phrase_embedings = tf.Variable(tf.random_uniform((number_of_words, embedding_size), -1, 1))
embedings = tf.nn.embedding_lookup(phrase_embedings, input_values)

lstm_cells = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size)
lstm_cells = tf.contrib.rnn.DropoutWrapper(lstm_cells, dropout_rate)
all_cell = tf.contrib.rnn.MultiRNNCell([lstm_cells] * number_of_layers)
first_state = all_cell.zero_state(batch_size, tf.float32)

feature_outputs, feature_states = tf.nn.dynamic_rnn(all_cell, embedings, initial_state=first_state)

prediction = tf.layers.dense(feature_outputs[:, -1], 1, activation=tf.sigmoid)
cost = tf.losses.mean_squared_error(target_values, prediction)

optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)

currect_pred = tf.equal(tf.cast(tf.round(prediction), tf.int32), target_values)
accuracy = tf.reduce_mean(tf.cast(currect_pred, tf.float32))

session = tf.Session()

session.run(tf.global_variables_initializer())

for i in range(epochs):
    training_accuracy = []
    iterator = 0
    epoch_loss = []
    while iterator + batch_size <= len(train_features):
        X_batch = train_features[iterator : iterator+batch_size]
        y_batch = train_labels[iterator : iterator+batch_size].reshape(-1, 1)
        a, o, _ = session.run([accuracy, cost, optimizer], feed_dict={input_values:X_batch, target_values:y_batch})
        training_accuracy.append(a)
        epoch_loss.append(o)
        iterator += batch_size
    print('Epoch: {}/{}'.format(i, epochs), ' | Current loss: {}'.format(np.mean(epoch_loss)))
    print('Training accuracy: {:.4f}'.format(np.mean(training_accurcy)*100))

test_accuracy = []

iterator = 0
while iterator + batch_size <= len(test_features):
    X_batch = X_test[iterator : iterator+batch_size]
    y_batch = y_test[iterator : iterator+batch_size].reshape(-1, 1)

    a = session.run([accuracy], feed_dict={input_values:X_batch, target_values:y_batch})
    
    test_accuracy.append(a)
    iterator += batch_size
print("Test accuracy = {:.4f}%".format(np.mean(test_accuracy)*100))

